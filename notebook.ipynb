{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T13:22:04.033508Z",
     "start_time": "2023-12-13T13:22:04.015378Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n\\nThere are eight planets in our Solar System: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.',\n",
       " 'As of now, there are eight recognized planets in our solar system. These include Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. However, the definition of a planet has been debated, and there may be other celestial bodies in our solar system that could potentially be classified as planets in the future.')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "# gpt 3.5 사용\n",
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "\n",
    "llm = OpenAI()\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "a = llm.predict(\"How many planets are there?\")\n",
    "b = chat.predict(\"How many planets are there?\")\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T13:22:35.160945Z",
     "start_time": "2023-12-13T13:22:31.992105Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "# temperature 모델의 창의성 수치를 정할수 있음\n",
    "# 값이 높을수록 창의성이 증가함, 원하는 값이 등장 안할수도 있음 무작위성 증가\n",
    "chat = ChatOpenAI(temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao! Il mio nome è Paolo. La distanza tra il Messico e la Thailandia è di circa 16.000 chilometri.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HumanMessage 는 사람이 보내는 메세지\n",
    "# AIMessage 는 ai에 의해 보내지는 것\n",
    "# SystemMessage는 시스템에 설정값을 보낼때 사용함\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a geography expert. And you only reply in Italian.\"),\n",
    "    AIMessage(content=\"Ciao, mi chiamo Paolo!\"),\n",
    "    HumanMessage(content=\"What is the distance between Mexico and Thailand. Also, what is your name?\")\n",
    "]\n",
    "# 변수들을 지정해서 원하는 값들 넣고 활용 \n",
    "# messages = [\n",
    "#     SystemMessage(content=\"You are a geography expert. And you only reply in {language}.\"),\n",
    "#     AIMessage(content=\"Ciao, mi chiamo {name}!\")\n",
    "#     HumanMessage(content=\"What is the distance between {contry} and Thailand. Also, what is your name?\")\n",
    "# ]\n",
    "\n",
    "chat.predict_messages(messages=messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I am an AI language model developed by OpenAI, and I don't have a personal name. You can simply refer to me as OpenAI Assistant. How can I assist you today?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.predict(\"Hello, what is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='30 plus 20 equals 50.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a teacher and present your score\"),\n",
    "    AIMessage(content=\"I am a GPT teacher!\"),\n",
    "    HumanMessage(content=\"Sir, what is 30 plus 20?\")\n",
    "]\n",
    "\n",
    "\n",
    "chat.predict_messages(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The distance between Mexico and South Korea is approximately 8,900 miles (14,320 kilometers) when measured in a straight line.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "# Prompt 기능 사용, 사용자가 오류됀 값을 입력하는걸 방지하기 위해, 아래처럼 두개의 단어만 넣는다\n",
    "template = PromptTemplate.from_template(\"What is the distance between {country_a} and {country_b}\")\n",
    "prompt = template.format(country_a=\"Maxico\", country_b=\"Korea\")\n",
    "\n",
    "# chat = ChatOpenAI()\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "chat.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Γεια σας! Η απόσταση μεταξύ του Μεξικού και της Ταϊλάνδης είναι περίπου 16.000 χιλιόμετρα. Το όνομά μου είναι Σωκράτης. Πώς μπορώ να βοηθήσω;')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a geography expert. And you only reply in {language}.\",\n",
    "        ),\n",
    "        (\n",
    "            \"ai\",\n",
    "            \"Ciao, mi chiamo {name}!\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"What is the distance between {country_a} and {country_b}. Also, what is your name?\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "prompt = messages.format_messages(\n",
    "    language=\"Greek\",\n",
    "    name=\"Socrates\",\n",
    "    country_a=\"Mexico\",\n",
    "    country_b=\"Thailand\",\n",
    ")\n",
    "\n",
    "chat.predict_messages(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'How', 'are you']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "\n",
    "    def parse(self, text):\n",
    "        items =  text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    \n",
    "p = CommaOutputParser()\n",
    "p.parse(\"Hello, How, are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Red',\n",
       " 'blue',\n",
       " 'green',\n",
       " 'yellow',\n",
       " 'orange',\n",
       " 'purple',\n",
       " 'pink',\n",
       " 'black',\n",
       " 'white',\n",
       " 'brown']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a list generating machine. Everything you are asked will be answered with a comma seperated list of max {max_items}. Do NOT reply with anything else.\",\n",
    "    ),\n",
    "    (\"human\", \"{question}\",),],\n",
    ")\n",
    "\n",
    "prompt = template.format_messages(max_items=10, question=\"What are the colors?\",)\n",
    "result = chat.predict_messages(prompt)\n",
    "\n",
    "# 리스트로 바꾸는 parser\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pikachu', 'Charizard', 'Bulbasaur', 'Squirtle', 'Jigglypuff']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이것이 체인\n",
    "chain_one = template | chat | CommaOutputParser()\n",
    "# chain_two = template | chat | CommaOutputParser()\n",
    "# all = chain_one | chain_two | parser 이렇게도 됌\n",
    "\n",
    "chain.invoke({\n",
    "    \"max_items\":5,\n",
    "    \"question\":\"What are the pokemons?\",\n",
    "},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# 스트리밍 작업\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "\n",
    "chef_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a world-class international chef. You create easy to follow recipies for any type of cuisine with easy to find ingreients.\",\n",
    "        ),\n",
    "        (\"human\", \"I want to cook {cuisine} food.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chef_chain = chef_prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great choice! Indian cuisine is known for its bold flavors and aromatic spices. Let's make a classic dish - Chicken Tikka Masala. Here's a simple recipe for you to try at home:\n",
      "\n",
      "Ingredients:\n",
      "- 1 lb boneless, skinless chicken breasts, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt\n",
      "- 2 tbsp lemon juice\n",
      "- 2 tbsp vegetable oil\n",
      "- 1 onion, finely chopped\n",
      "- 3 cloves garlic, minced\n",
      "- 1-inch piece of ginger, grated\n",
      "- 1 can (14 oz) crushed tomatoes\n",
      "- 1 tbsp garam masala\n",
      "- 1 tsp ground cumin\n",
      "- 1 tsp ground coriander\n",
      "- 1 tsp turmeric\n",
      "- 1/2 tsp cayenne pepper (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (to serve)\n",
      "\n",
      "Instructions:\n",
      "1. In a bowl, mix together the yogurt, lemon juice, 1 tbsp vegetable oil, half of the minced garlic, half of the grated ginger, 1 tsp garam masala, 1/2 tsp cumin, 1/2 tsp coriander, 1/2 tsp turmeric, cayenne pepper, salt, and pepper. Add the chicken pieces and marinate for at least 1 hour, preferably overnight.\n",
      "\n",
      "2. Preheat the oven to 400°F (200°C). Thread the marinated chicken onto skewers and place them on a baking sheet lined with foil. Bake for 20-25 minutes or until the chicken is cooked through.\n",
      "\n",
      "3. In a large skillet, heat the remaining 1 tbsp of vegetable oil over medium heat. Add the chopped onion and cook until softened, about 5 minutes. Add the remaining garlic and ginger and cook for another minute.\n",
      "\n",
      "4. Stir in the crushed tomatoes, remaining garam masala, cumin, coriander, and turmeric. Simmer for 10-15 minutes, stirring occasionally.\n",
      "\n",
      "5. Add the cooked chicken tikka to the skillet and simmer for an additional 5-10 minutes to allow the flavors to meld together. Adjust seasoning with salt and pepper as needed.\n",
      "\n",
      "6. Serve the Chicken Tikka Masala over cooked rice or with naan bread. Garnish with chopped cilantro. Enjoy your delicious homemade Indian meal!\n",
      "\n",
      "Feel free to customize this recipe by adding more vegetables or adjusting the spice level to suit your taste preferences. Enjoy your culinary adventure in the world of Indian cuisine!As a vegetarian chef, I can help you make a vegetarian version of Chicken Tikka Masala by replacing the chicken with a suitable alternative. Here's how you can modify the recipe:\n",
      "\n",
      "Alternative Ingredient:\n",
      "- Instead of chicken, you can use paneer (Indian cottage cheese) or tofu as a vegetarian protein option. Both paneer and tofu have a mild flavor that absorbs the spices well, making them great substitutes for chicken in this dish.\n",
      "\n",
      "Preparation:\n",
      "- If using paneer: Cut the paneer into bite-sized cubes and follow the same marination process as you would with the chicken. Paneer doesn't need as much time to marinate, so 30 minutes should be sufficient.\n",
      "- If using tofu: Drain the tofu well and cut it into bite-sized pieces. To give the tofu a firmer texture, you can press it between paper towels with a weight on top for about 15-20 minutes before marinating. Tofu can be more delicate, so handle it gently during the marination process.\n",
      "\n",
      "Follow the rest of the recipe as it is, substituting the marinated paneer or tofu for the chicken when baking and simmering in the sauce. The paneer or tofu will absorb the flavors of the spices and yogurt, resulting in a delicious vegetarian version of Chicken Tikka Masala.\n",
      "\n",
      "Enjoy your vegetarian twist on this classic Indian dish!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=\"As a vegetarian chef, I can help you make a vegetarian version of Chicken Tikka Masala by replacing the chicken with a suitable alternative. Here's how you can modify the recipe:\\n\\nAlternative Ingredient:\\n- Instead of chicken, you can use paneer (Indian cottage cheese) or tofu as a vegetarian protein option. Both paneer and tofu have a mild flavor that absorbs the spices well, making them great substitutes for chicken in this dish.\\n\\nPreparation:\\n- If using paneer: Cut the paneer into bite-sized cubes and follow the same marination process as you would with the chicken. Paneer doesn't need as much time to marinate, so 30 minutes should be sufficient.\\n- If using tofu: Drain the tofu well and cut it into bite-sized pieces. To give the tofu a firmer texture, you can press it between paper towels with a weight on top for about 15-20 minutes before marinating. Tofu can be more delicate, so handle it gently during the marination process.\\n\\nFollow the rest of the recipe as it is, substituting the marinated paneer or tofu for the chicken when baking and simmering in the sauce. The paneer or tofu will absorb the flavors of the spices and yogurt, resulting in a delicious vegetarian version of Chicken Tikka Masala.\\n\\nEnjoy your vegetarian twist on this classic Indian dish!\")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모르겠다면 모르겠다고 말하는걸 세팅하는게 아주 중요함\n",
    "veg_chef_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a vegetarian chef specialized on making traditional recipies vegetarian. You find alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative for a food just say you don't know how to replace it.\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"{recipe}\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "veg_chain = veg_chef_prompt | chat\n",
    "\n",
    "# recipe 를 담아\n",
    "final_chain = {\"recipe\" : chef_chain} | veg_chain\n",
    "\n",
    "\n",
    "final_chain.invoke(\n",
    "    {\n",
    "    \"cuisine\":\"indian\",\n",
    "    },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of France'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# 스트리밍 작업\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler(),],\n",
    ")\n",
    "\n",
    "t = PromptTemplate(\n",
    "    template=\"What is the capital of {country}\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "t.format(country=\"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Franch is a term that is often used to refer to the French language or culture. It is a colloquial term that is sometimes used in a humorous or exaggerated way to describe things that are associated with France or French culture. It is not a formal or official term, but rather a slang term that is used in casual conversation."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Franch is a term that is often used to refer to the French language or culture. It is a colloquial term that is sometimes used in a humorous or exaggerated way to describe things that are associated with France or French culture. It is not a formal or official term, but rather a slang term that is used in casual conversation.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FewShot이란 모델에게 예제들을 주는것\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "                    Here is what I know:\n",
    "                    Capital: Paris\n",
    "                    Language: French\n",
    "                    Food: Wine and Cheese\n",
    "                    Currency: Euro\n",
    "                    \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "                    I know this:\n",
    "                    Capital: Rome\n",
    "                    Language: Italian\n",
    "                    Food: Pizza and Pasta\n",
    "                    Currency: Euro\n",
    "                    \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "                I know this:\n",
    "                Capital: Athens\n",
    "                Language: Greek\n",
    "                Food: Souvlaki and Feta Cheese\n",
    "                Currency: Euro\n",
    "                \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "chat.predict(\"What do you know about Franch?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n",
      "                I know this:\n",
      "                Capital: Berlin\n",
      "                Language: German\n",
      "                Food: Bratwurst and Sauerkraut\n",
      "                Currency: Euro"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='AI: \\n                I know this:\\n                Capital: Berlin\\n                Language: German\\n                Food: Bratwurst and Sauerkraut\\n                Currency: Euro')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_template = \"\"\"\n",
    "    Human: {question}\n",
    "    AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# 2개다 상관없음\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "# prompt.format(country=\"Germany\")\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"country\":\"Germany\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I know this:\n",
      "Capital: Berlin\n",
      "Language: German\n",
      "Food: Bratwurst and Sauerkraut\n",
      "Currency: Euro"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='I know this:\\nCapital: Berlin\\nLanguage: German\\nFood: Bratwurst and Sauerkraut\\nCurrency: Euro')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France?\",\n",
    "        \"answer\": \"\"\"\n",
    "                    Here is what I know:\n",
    "                    Capital: Paris\n",
    "                    Language: French\n",
    "                    Food: Wine and Cheese\n",
    "                    Currency: Euro\n",
    "                    \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "                    I know this:\n",
    "                    Capital: Rome\n",
    "                    Language: Italian\n",
    "                    Food: Pizza and Pasta\n",
    "                    Currency: Euro\n",
    "                    \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "                I know this:\n",
    "                Capital: Athens\n",
    "                Language: Greek\n",
    "                Food: Souvlaki and Feta Cheese\n",
    "                Currency: Euro\n",
    "                \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"What do you know about {country}?\"),\n",
    "    (\"ai\", \"{answer}\")\n",
    "])\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a geography exper\"),\n",
    "        example_prompt,\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt |  chat\n",
    "\n",
    "chain.invoke({\"country\":\"Germany\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What do you know about Greece?\\nAI:\\n        I know this:\\n        Capital: Athens\\n        Language: Greek\\n        Food: Souvlaki and Feta Cheese\\n        Currency: Euro\\n        \\n\\nHuman: What do you know about Brazil?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import example_selector\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of Germany'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"./prompt.json\")\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Germany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr matey! Me favorite grub be a hearty plate of salted beef and hardtack biscuits! Aye, nothing beats the taste of the sea on me tongue! Arrr!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='Arrr matey! Me favorite grub be a hearty plate of salted beef and hardtack biscuits! Aye, nothing beats the taste of the sea on me tongue! Arrr!')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "                                     \n",
    "    {example}\n",
    "                              \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start)\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final, \n",
    "    pipeline_prompts=prompts,\n",
    "    )\n",
    "# full_prompt.format(\n",
    "#     character=\"Pirate\",\n",
    "#     example_question=\"What is your location?\",\n",
    "#     example_answer=\"Arrrrg! That is a secret!! Arg arg!\",\n",
    "#     question=\"What is your fav food?\",\n",
    "# )\n",
    "\n",
    "chain = full_prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"character\":\"Pirate\",\n",
    "    \"example_question\":\"What is your location?\",\n",
    "    \"example_answer\":\"Arrrrg! That is a secret!! Arg arg!\",\n",
    "    \"question\":\"What is your fav food?\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: How do you make italian pasta\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [3.78s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- Pinch of salt\\n- Water (if needed)\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and create a well in the center.\\n2. Crack the eggs into the well and add a pinch of salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it becomes smooth and elastic. If the dough is too dry, add a little water. If it is too wet, add a little more flour.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine, spaghetti, or ravioli.\\n8. Cook the pasta in a large pot of salted boiling water for 2-3 minutes or until al dente.\\n9. Drain the pasta and toss it with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- Pinch of salt\\n- Water (if needed)\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and create a well in the center.\\n2. Crack the eggs into the well and add a pinch of salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it becomes smooth and elastic. If the dough is too dry, add a little water. If it is too wet, add a little more flour.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine, spaghetti, or ravioli.\\n8. Cook the pasta in a large pot of salted boiling water for 2-3 minutes or until al dente.\\n9. Drain the pasta and toss it with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 13,\n",
      "      \"completion_tokens\": 258,\n",
      "      \"total_tokens\": 271\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": \"fp_4f2ebda25a\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- Pinch of salt\\n- Water (if needed)\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and create a well in the center.\\n2. Crack the eggs into the well and add a pinch of salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it becomes smooth and elastic. If the dough is too dry, add a little water. If it is too wet, add a little more flour.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine, spaghetti, or ravioli.\\n8. Cook the pasta in a large pot of salted boiling water for 2-3 minutes or until al dente.\\n9. Drain the pasta and toss it with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caching\n",
    "# 똑같은 질문을 받아도 같은 답변을 줌으로써 반복 해결\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "# set_llm_cache(InMemoryCache())\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "set_debug(True)\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "chat.predict(\"How do you make italian pasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- 1/2 teaspoon of salt\\n- Water (if needed)\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and make a well in the center.\\n2. Crack the eggs into the well and add the salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it is smooth and elastic. If the dough is too dry, add a little water. If it is too wet, add a little more flour.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes, or until al dente.\\n9. Drain the pasta and toss it with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.predict(\"How do you make italian pasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: What is the recipe for soju\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [2ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Ingredients:\\n- 1 cup of rice\\n- 1 cup of water\\n- 1 tablespoon of nuruk (fermentation starter)\\n- 1 tablespoon of sugar\\n\\nInstructions:\\n1. Rinse the rice thoroughly and soak it in water for at least 1 hour.\\n2. Drain the rice and steam it until it is fully cooked.\\n3. In a large bowl, mix the cooked rice with water and nuruk.\\n4. Cover the bowl with a clean cloth and let it ferment in a warm place for 3-4 days.\\n5. After fermentation, strain the mixture through a cheesecloth to remove any solids.\\n6. Add sugar to the strained liquid and mix well.\\n7. Transfer the liquid to a clean bottle and let it sit for another 1-2 days to allow the flavors to develop.\\n8. Serve the homemade soju chilled and enjoy!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Ingredients:\\n- 1 cup of rice\\n- 1 cup of water\\n- 1 tablespoon of nuruk (fermentation starter)\\n- 1 tablespoon of sugar\\n\\nInstructions:\\n1. Rinse the rice thoroughly and soak it in water for at least 1 hour.\\n2. Drain the rice and steam it until it is fully cooked.\\n3. In a large bowl, mix the cooked rice with water and nuruk.\\n4. Cover the bowl with a clean cloth and let it ferment in a warm place for 3-4 days.\\n5. After fermentation, strain the mixture through a cheesecloth to remove any solids.\\n6. Add sugar to the strained liquid and mix well.\\n7. Transfer the liquid to a clean bottle and let it sit for another 1-2 days to allow the flavors to develop.\\n8. Serve the homemade soju chilled and enjoy!\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: What is the recipe for bread\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [5.73s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Ingredients:\\n- 3 cups all-purpose flour\\n- 1 packet active dry yeast\\n- 1 1/4 cups warm water\\n- 1 tablespoon sugar\\n- 1 teaspoon salt\\n- 2 tablespoons olive oil\\n\\nInstructions:\\n1. In a small bowl, combine the warm water, sugar, and yeast. Let it sit for about 5-10 minutes until it becomes frothy.\\n2. In a large mixing bowl, combine the flour and salt. Make a well in the center and pour in the yeast mixture and olive oil.\\n3. Mix the ingredients together until a dough forms. Knead the dough on a floured surface for about 5-10 minutes until it becomes smooth and elastic.\\n4. Place the dough in a greased bowl, cover it with a clean towel, and let it rise in a warm place for about 1-2 hours or until it has doubled in size.\\n5. Preheat the oven to 375°F (190°C). Punch down the dough and shape it into a loaf. Place the loaf on a baking sheet lined with parchment paper.\\n6. Cover the loaf with a clean towel and let it rise for another 30 minutes.\\n7. Bake the bread in the preheated oven for about 25-30 minutes or until it is golden brown and sounds hollow when tapped on the bottom.\\n8. Let the bread cool before slicing and serving. Enjoy your freshly baked bread!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Ingredients:\\n- 3 cups all-purpose flour\\n- 1 packet active dry yeast\\n- 1 1/4 cups warm water\\n- 1 tablespoon sugar\\n- 1 teaspoon salt\\n- 2 tablespoons olive oil\\n\\nInstructions:\\n1. In a small bowl, combine the warm water, sugar, and yeast. Let it sit for about 5-10 minutes until it becomes frothy.\\n2. In a large mixing bowl, combine the flour and salt. Make a well in the center and pour in the yeast mixture and olive oil.\\n3. Mix the ingredients together until a dough forms. Knead the dough on a floured surface for about 5-10 minutes until it becomes smooth and elastic.\\n4. Place the dough in a greased bowl, cover it with a clean towel, and let it rise in a warm place for about 1-2 hours or until it has doubled in size.\\n5. Preheat the oven to 375°F (190°C). Punch down the dough and shape it into a loaf. Place the loaf on a baking sheet lined with parchment paper.\\n6. Cover the loaf with a clean towel and let it rise for another 30 minutes.\\n7. Bake the bread in the preheated oven for about 25-30 minutes or until it is golden brown and sounds hollow when tapped on the bottom.\\n8. Let the bread cool before slicing and serving. Enjoy your freshly baked bread!\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 13,\n",
      "      \"completion_tokens\": 295,\n",
      "      \"total_tokens\": 308\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": \"fp_4f2ebda25a\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "Ingredients:\n",
      "- 1 cup of rice\n",
      "- 1 cup of water\n",
      "- 1 tablespoon of nuruk (fermentation starter)\n",
      "- 1 tablespoon of sugar\n",
      "\n",
      "Instructions:\n",
      "1. Rinse the rice thoroughly and soak it in water for at least 1 hour.\n",
      "2. Drain the rice and steam it until it is fully cooked.\n",
      "3. In a large bowl, mix the cooked rice with water and nuruk.\n",
      "4. Cover the bowl with a clean cloth and let it ferment in a warm place for 3-4 days.\n",
      "5. After fermentation, strain the mixture through a cheesecloth to remove any solids.\n",
      "6. Add sugar to the strained liquid and mix well.\n",
      "7. Transfer the liquid to a clean bottle and let it sit for another 1-2 days to allow the flavors to develop.\n",
      "8. Serve the homemade soju chilled and enjoy! Ingredients:\n",
      "- 3 cups all-purpose flour\n",
      "- 1 packet active dry yeast\n",
      "- 1 1/4 cups warm water\n",
      "- 1 tablespoon sugar\n",
      "- 1 teaspoon salt\n",
      "- 2 tablespoons olive oil\n",
      "\n",
      "Instructions:\n",
      "1. In a small bowl, combine the warm water, sugar, and yeast. Let it sit for about 5-10 minutes until it becomes frothy.\n",
      "2. In a large mixing bowl, combine the flour and salt. Make a well in the center and pour in the yeast mixture and olive oil.\n",
      "3. Mix the ingredients together until a dough forms. Knead the dough on a floured surface for about 5-10 minutes until it becomes smooth and elastic.\n",
      "4. Place the dough in a greased bowl, cover it with a clean towel, and let it rise in a warm place for about 1-2 hours or until it has doubled in size.\n",
      "5. Preheat the oven to 375°F (190°C). Punch down the dough and shape it into a loaf. Place the loaf on a baking sheet lined with parchment paper.\n",
      "6. Cover the loaf with a clean towel and let it rise for another 30 minutes.\n",
      "7. Bake the bread in the preheated oven for about 25-30 minutes or until it is golden brown and sounds hollow when tapped on the bottom.\n",
      "8. Let the bread cool before slicing and serving. Enjoy your freshly baked bread! \n",
      "\n",
      "Tokens Used: 308\n",
      "\tPrompt Tokens: 13\n",
      "\tCompletion Tokens: 295\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0006095\n"
     ]
    }
   ],
   "source": [
    "# 비용 계산 및 사용 토큰 계산\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"What is the recipe for soju\")\n",
    "    b = chat.predict(\"What is the recipe for bread\")\n",
    "    print(a, b, \"\\n\")\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "\n",
    "chat = OpenAI(\n",
    "    temperature=0.1,\n",
    "    max_tokens=450,\n",
    "    model=\"gpt-3.5-turbo-16k\",\n",
    ")\n",
    "# 모델 저장\n",
    "chat.save(\"model.json\")\n",
    "\n",
    "# 모델 불러오기\n",
    "chat = load_llm(\"model.json\")\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!'), AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메모리 기억, 전체를 기억하는 타입이라 용량이 매우큼, 비효율적\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "#챗모델이면 return messages True로 하면 대화 내용으로 나옴\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\":\"Hi!\"}, {\"output\":\"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화 버퍼 윈도우 메모리\n",
    "# 대화의 특정 부분만을 저장하는 메모리\n",
    "# 단점은 전체대화가 아닌 최근꺼만 기억함\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4,\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1'),\n",
       "  AIMessage(content='1'),\n",
       "  HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4')]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5')]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"summary\": \"\",\n",
      "  \"new_lines\": \"Human: Hi I'm Seunggyu, I live in South Korea\\nAI: Wow that is so cool!\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n\\n\\nNew lines of conversation:\\nHuman: Hi I'm Seunggyu, I live in South Korea\\nAI: Wow that is so cool!\\n\\nNew summary:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] [1.25s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 158,\n",
      "      \"completion_tokens\": 22,\n",
      "      \"total_tokens\": 180\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": \"fp_4f2ebda25a\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [1.25s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# SummaryMemory, LLM을 이용한 기억\n",
    "# 메모리를 사용하여 요금이 좀 나감, 메모리 요약을 아주 잘해줌, 길수록 유용함\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Seunggyu, I live in South Korea\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"summary\": \"The human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information.\",\n",
      "  \"new_lines\": \"Human: South Korea is so pretty\\nAI: I wish I could go!\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\nThe human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information.\\n\\nNew lines of conversation:\\nHuman: South Korea is so pretty\\nAI: I wish I could go!\\n\\nNew summary:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] [1.44s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information, expressing a desire to visit the country because it is so pretty.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information, expressing a desire to visit the country because it is so pretty.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 172,\n",
      "      \"completion_tokens\": 35,\n",
      "      \"total_tokens\": 207\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": \"fp_4f2ebda25a\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [1.44s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information, expressing a desire to visit the country because it is so pretty.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "add_message(\"South Korea is so pretty\", \"I wish I could go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information, expressing a desire to visit the country because it is so pretty.'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"summary\": \"\",\n",
      "  \"new_lines\": \"Human: Hi I'm Seunggyu, I live in South Korea\\nAI: Wow that is so cool!\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n\\n\\nNew lines of conversation:\\nHuman: Hi I'm Seunggyu, I live in South Korea\\nAI: Wow that is so cool!\\n\\nNew summary:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] [4ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [5ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# memory, 최근의 상호작용은 추적, 오래됀 메세지를 요약해서 저장\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm, max_token_limit=150,\n",
    "                                   return_messages=True,)\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm Seunggyu, I live in South Korea\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information.'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"summary\": \"The human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information.\",\n",
      "  \"new_lines\": \"Human: South Korea is so pretty\\nAI: I wish I could go!\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\nThe human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information.\\n\\nNew lines of conversation:\\nHuman: South Korea is so pretty\\nAI: I wish I could go!\\n\\nNew summary:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] [2ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information, expressing a desire to visit the country because it is so pretty.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information, expressing a desire to visit the country because it is so pretty.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information, expressing a desire to visit the country because it is so pretty.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "add_message(\"South Korea is so pretty\", \"I wish I could go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces themselves as Seunggyu from South Korea. The AI responds with enthusiasm at the information, expressing a desire to visit the country because it is so pretty.'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"history\": \"Human: Hi I'm Seunggyu, I live in South Korea\\nAI: Wow that is so cool!\",\n",
      "  \"input\": \"Hi I'm Seunggyu, I live in South Korea\",\n",
      "  \"verbose\": true\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a networked intelligence helping a human track knowledge triples about all relevant people, things, concepts, etc. and integrating them with your knowledge stored within your weights as well as that stored in a knowledge graph. Extract all of the knowledge triples from the last line of conversation. A knowledge triple is a clause that contains a subject, a predicate, and an object. The subject is the entity being described, the predicate is the property of the subject that is being described, and the object is the value of the property.\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: Did you hear aliens landed in Area 51?\\nAI: No, I didn't hear that. What do you know about Area 51?\\nPerson #1: It's a secret military base in Nevada.\\nAI: What do you know about Nevada?\\nLast line of conversation:\\nPerson #1: It's a state in the US. It's also the number 1 producer of gold in the US.\\n\\nOutput: (Nevada, is a, state)<|>(Nevada, is in, US)<|>(Nevada, is the number 1 producer of, gold)\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: Hello.\\nAI: Hi! How are you?\\nPerson #1: I'm good. How are you?\\nAI: I'm good too.\\nLast line of conversation:\\nPerson #1: I'm going to the store.\\n\\nOutput: NONE\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: What do you know about Descartes?\\nAI: Descartes was a French philosopher, mathematician, and scientist who lived in the 17th century.\\nPerson #1: The Descartes I'm referring to is a standup comedian and interior designer from Montreal.\\nAI: Oh yes, He is a comedian and an interior designer. He has been in the industry for 30 years. His favorite food is baked bean pie.\\nLast line of conversation:\\nPerson #1: Oh huh. I know Descartes likes to drive antique scooters and play the mandolin.\\nOutput: (Descartes, likes to drive, antique scooters)<|>(Descartes, plays, mandolin)\\nEND OF EXAMPLE\\n\\nConversation history (for reference only):\\nHuman: Hi I'm Seunggyu, I live in South Korea\\nAI: Wow that is so cool!\\nLast line of conversation (for extraction):\\nHuman: Hi I'm Seunggyu, I live in South Korea\\n\\nOutput:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] [1.10s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"(Seunggyu, lives in, South Korea)\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"(Seunggyu, lives in, South Korea)\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 522,\n",
      "      \"completion_tokens\": 12,\n",
      "      \"total_tokens\": 534\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": \"fp_4f2ebda25a\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [1.10s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"(Seunggyu, lives in, South Korea)\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# COnversationKGMemory\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# 가장 중요한것만 뽑아내는 것\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm Seunggyu, I live in South Korea\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"history\": \"Human: Hi I'm Seunggyu, I live in South Korea\\nAI: Wow that is so cool!\",\n",
      "  \"input\": \"who is Seunggyu\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an AI assistant reading the transcript of a conversation between an AI and a human. Extract all of the proper nouns from the last line of conversation. As a guideline, a proper noun is generally capitalized. You should definitely extract all names and places.\\n\\nThe conversation history is provided just in case of a coreference (e.g. \\\"What do you know about him\\\" where \\\"him\\\" is defined in a previous line) -- ignore items mentioned there that are not in the last line.\\n\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return (e.g. the user is just issuing a greeting or having a simple conversation).\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: how's it going today?\\nAI: \\\"It's going great! How about you?\\\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \\\"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\\\"\\nLast line:\\nPerson #1: i'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\nOutput: Langchain\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: how's it going today?\\nAI: \\\"It's going great! How about you?\\\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \\\"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\\\"\\nLast line:\\nPerson #1: i'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I'm working with Person #2.\\nOutput: Langchain, Person #2\\nEND OF EXAMPLE\\n\\nConversation history (for reference only):\\nHuman: Hi I'm Seunggyu, I live in South Korea\\nAI: Wow that is so cool!\\nLast line of conversation (for extraction):\\nHuman: who is Seunggyu\\n\\nOutput:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] [772ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Seunggyu\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Seunggyu\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 429,\n",
      "      \"completion_tokens\": 4,\n",
      "      \"total_tokens\": 433\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": \"fp_4f2ebda25a\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [773ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Seunggyu\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Seunggyu: Seunggyu lives in South Korea.')]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\":\"who is Seunggyu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"history\": \"Human: Hi I'm Seunggyu, I live in South Korea\\nAI: Wow that is so cool!\\nHuman: Seunggyu likes kimchi\\nAI: Wow that is so cool!\",\n",
      "  \"input\": \"Seunggyu likes kimchi\",\n",
      "  \"verbose\": true\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a networked intelligence helping a human track knowledge triples about all relevant people, things, concepts, etc. and integrating them with your knowledge stored within your weights as well as that stored in a knowledge graph. Extract all of the knowledge triples from the last line of conversation. A knowledge triple is a clause that contains a subject, a predicate, and an object. The subject is the entity being described, the predicate is the property of the subject that is being described, and the object is the value of the property.\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: Did you hear aliens landed in Area 51?\\nAI: No, I didn't hear that. What do you know about Area 51?\\nPerson #1: It's a secret military base in Nevada.\\nAI: What do you know about Nevada?\\nLast line of conversation:\\nPerson #1: It's a state in the US. It's also the number 1 producer of gold in the US.\\n\\nOutput: (Nevada, is a, state)<|>(Nevada, is in, US)<|>(Nevada, is the number 1 producer of, gold)\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: Hello.\\nAI: Hi! How are you?\\nPerson #1: I'm good. How are you?\\nAI: I'm good too.\\nLast line of conversation:\\nPerson #1: I'm going to the store.\\n\\nOutput: NONE\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: What do you know about Descartes?\\nAI: Descartes was a French philosopher, mathematician, and scientist who lived in the 17th century.\\nPerson #1: The Descartes I'm referring to is a standup comedian and interior designer from Montreal.\\nAI: Oh yes, He is a comedian and an interior designer. He has been in the industry for 30 years. His favorite food is baked bean pie.\\nLast line of conversation:\\nPerson #1: Oh huh. I know Descartes likes to drive antique scooters and play the mandolin.\\nOutput: (Descartes, likes to drive, antique scooters)<|>(Descartes, plays, mandolin)\\nEND OF EXAMPLE\\n\\nConversation history (for reference only):\\nHuman: Hi I'm Seunggyu, I live in South Korea\\nAI: Wow that is so cool!\\nHuman: Seunggyu likes kimchi\\nAI: Wow that is so cool!\\nLast line of conversation (for extraction):\\nHuman: Seunggyu likes kimchi\\n\\nOutput:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] [1.05s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"(Seunggyu, likes, kimchi)\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"(Seunggyu, likes, kimchi)\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 534,\n",
      "      \"completion_tokens\": 11,\n",
      "      \"total_tokens\": 545\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": \"fp_4f2ebda25a\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [1.05s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"(Seunggyu, likes, kimchi)\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "add_message(\"Seunggyu likes kimchi\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"history\": \"Human: Hi I'm Seunggyu, I live in South Korea\\nAI: Wow that is so cool!\\nHuman: Seunggyu likes kimchi\\nAI: Wow that is so cool!\",\n",
      "  \"input\": \"what does Seunggyu like\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an AI assistant reading the transcript of a conversation between an AI and a human. Extract all of the proper nouns from the last line of conversation. As a guideline, a proper noun is generally capitalized. You should definitely extract all names and places.\\n\\nThe conversation history is provided just in case of a coreference (e.g. \\\"What do you know about him\\\" where \\\"him\\\" is defined in a previous line) -- ignore items mentioned there that are not in the last line.\\n\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return (e.g. the user is just issuing a greeting or having a simple conversation).\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: how's it going today?\\nAI: \\\"It's going great! How about you?\\\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \\\"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\\\"\\nLast line:\\nPerson #1: i'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\nOutput: Langchain\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: how's it going today?\\nAI: \\\"It's going great! How about you?\\\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \\\"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\\\"\\nLast line:\\nPerson #1: i'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I'm working with Person #2.\\nOutput: Langchain, Person #2\\nEND OF EXAMPLE\\n\\nConversation history (for reference only):\\nHuman: Hi I'm Seunggyu, I live in South Korea\\nAI: Wow that is so cool!\\nHuman: Seunggyu likes kimchi\\nAI: Wow that is so cool!\\nLast line of conversation (for extraction):\\nHuman: what does Seunggyu like\\n\\nOutput:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] [879ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Seunggyu\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Seunggyu\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 448,\n",
      "      \"completion_tokens\": 4,\n",
      "      \"total_tokens\": 452\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": \"fp_4f2ebda25a\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [880ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Seunggyu\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Seunggyu: Seunggyu lives in South Korea. Seunggyu likes kimchi.')]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\":\"what does Seunggyu like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    \n",
      "    Human:My name is Seunggyu\n",
      "    YOu:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Seunggyu! How can I assist you today?'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# memory on Chain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\"\n",
    ")\n",
    "# template 안에랑 이름은 일치해야함\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    YOu:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    # prompt=PromptTemplate.from_template(\"{question}\"),\n",
    "    prompt=PromptTemplate.from_template(template=template),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Seunggyu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Seunggyu\n",
      "AI: Hello Seunggyu! How can I assist you today?\n",
      "    Human:I live in Seoul\n",
      "    YOu:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great to know! How can I assist you with living in Seoul today?\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Seunggyu\n",
      "AI: Hello Seunggyu! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great to know! How can I assist you with living in Seoul today?\n",
      "    Human:What is my name?\n",
      "    YOu:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Seunggyu.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': \"Human: My name is Seunggyu\\nAI: Hello Seunggyu! How can I assist you today?\\nHuman: I live in Seoul\\nAI: That's great to know! How can I assist you with living in Seoul today?\\nHuman: What is my name?\\nAI: Your name is Seunggyu.\"}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat 베이스 메모리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Seunggyu\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Seunggyu! How can I assist you today?'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Seunggyu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Seunggyu\n",
      "AI: Nice to meet you, Seunggyu! How can I assist you today?\n",
      "Human: I live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Seunggyu\n",
      "AI: Nice to meet you, Seunggyu! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?\n",
      "Human: What is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Seunggyu.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL Based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(input):\n",
    "    print(input)\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke(\n",
    "        {\n",
    "            \"question\": question,\n",
    "        }\n",
    "    )\n",
    "    memory.save_context({\"input\":question}, {\"output\": result.content})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'My name is Seunggyu'}\n",
      "content='Nice to meet you, Seunggyu! How can I assist you today?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is Seunggyu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is my name?'}\n",
      "content='Your name is Seunggyu.'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
